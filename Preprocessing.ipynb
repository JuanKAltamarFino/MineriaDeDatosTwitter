{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "#!pip install seaborn\n",
    "import seaborn as sns\n",
    "import string\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_tweets.csv',encoding = 'latin',names=[\"target\",\"id\",\"date\",\"Query\",\"user\",\"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200001, 7)\n",
      "(12234, 7)\n",
      "(0, 7)\n",
      "(0, 7)\n"
     ]
    }
   ],
   "source": [
    "#Partir el dataset\n",
    "df['preprocess']=''\n",
    "df.head()\n",
    "df1 = df.loc[:200000,:]\n",
    "df2 = df.loc[400000:799999,:]\n",
    "df3 = df.loc[800000:1199999,:]\n",
    "df4 = df.loc[1200000:,:]\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n",
    "print(df4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    412234\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped.count()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#Limpieza basica del tweet\n",
    "def processTweet(tweet):\n",
    "    # process the tweets\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.str.lower()\n",
    "    #Convert www.* or https?://* to URL\n",
    "    tweet = tweet.str.replace('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', regex=True)\n",
    "    #tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = tweet.str.replace('@[^\\s]+', 'AT_USER', regex=True)\n",
    "    #tweet = re.sub('@[^\\s]+','AT_USER',tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = tweet.str.replace('[\\s]+', ' ', regex=True)\n",
    "    #tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = tweet.str.replace(r'#([^\\s]+)', r'\\1', regex=True)\n",
    "    #tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #tweet = tweet.str.replace(\"[^a-zA-Z#]\",\"SPECIAL\")\n",
    "    #trim\n",
    "    tweet = tweet.str.strip('\\'\"')\n",
    "    return tweet\n",
    "\n",
    "df1.loc[:,'preprocess'] =processTweet(df1.loc[:,'tweet'])\n",
    "df2.loc[:,'preprocess'] =processTweet(df2.loc[:,'tweet'])\n",
    "df3.loc[:,'preprocess'] =processTweet(df3.loc[:,'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JK: YA no se requiere\n",
    "#Metodo que remueve un patr√≥n usado para quitar las menciones @user\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, 'AT_USER', input_txt)    \n",
    "    return input_txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se elminan las menciones del dataset\n",
    "#df1.loc[:,'preprocess'] = np.vectorize(remove_pattern)(df1['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.loc[:,'preprocess'] = np.vectorize(remove_pattern)(df2['tweet'], \"@[\\w]*\")\n",
    "#df3.loc[:,'preprocess'] = np.vectorize(remove_pattern)(df3['tweet'], \"@[\\w]*\")\n",
    "#df4.loc[:,'preprocess'] = np.vectorize(remove_pattern)(df4['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>AT_USER URL - awww, that's a bummer. you shoul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>AT_USER i dived many times for the ball. manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>AT_USER no, it's not behaving at all. i'm mad....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                          preprocess  \n",
       "0  AT_USER URL - awww, that's a bummer. you shoul...  \n",
       "1  is upset that he can't update his facebook by ...  \n",
       "2  AT_USER i dived many times for the ball. manag...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  AT_USER no, it's not behaving at all. i'm mad....  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SE USA PARA VISUALIZAR EL DATASET ANTES Y DESPUES DEL PREPROCESS\n",
    "df1.head().loc[:,\"tweet\":\"preprocess\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>400000</th>\n",
       "      <td>0</td>\n",
       "      <td>2057298217</td>\n",
       "      <td>Sat Jun 06 12:44:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kannibalKoi</td>\n",
       "      <td>so mutha effin bored</td>\n",
       "      <td>so mutha effin bored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400001</th>\n",
       "      <td>0</td>\n",
       "      <td>2057298537</td>\n",
       "      <td>Sat Jun 06 12:44:33 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TMankin</td>\n",
       "      <td>@CallaLilies83 not at all...I've spent most of...</td>\n",
       "      <td>AT_USER not at all...i've spent most of the af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400002</th>\n",
       "      <td>0</td>\n",
       "      <td>2057298663</td>\n",
       "      <td>Sat Jun 06 12:44:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>theresa_jx</td>\n",
       "      <td>@GiGisOssum :/ My entire friggin family?  Ther...</td>\n",
       "      <td>AT_USER :/ my entire friggin family? there's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400003</th>\n",
       "      <td>0</td>\n",
       "      <td>2057298745</td>\n",
       "      <td>Sat Jun 06 12:44:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mishellfish</td>\n",
       "      <td>I absolutely &amp;lt;3 sleeping in on saturdays. I...</td>\n",
       "      <td>i absolutely &amp;lt;3 sleeping in on saturdays. i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400004</th>\n",
       "      <td>0</td>\n",
       "      <td>2057298766</td>\n",
       "      <td>Sat Jun 06 12:44:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>meganleahhall</td>\n",
       "      <td>is a little upset at the moment</td>\n",
       "      <td>is a little upset at the moment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                          date     Query  \\\n",
       "400000       0  2057298217  Sat Jun 06 12:44:31 PDT 2009  NO_QUERY   \n",
       "400001       0  2057298537  Sat Jun 06 12:44:33 PDT 2009  NO_QUERY   \n",
       "400002       0  2057298663  Sat Jun 06 12:44:34 PDT 2009  NO_QUERY   \n",
       "400003       0  2057298745  Sat Jun 06 12:44:34 PDT 2009  NO_QUERY   \n",
       "400004       0  2057298766  Sat Jun 06 12:44:34 PDT 2009  NO_QUERY   \n",
       "\n",
       "                 user                                              tweet  \\\n",
       "400000    kannibalKoi                              so mutha effin bored    \n",
       "400001        TMankin  @CallaLilies83 not at all...I've spent most of...   \n",
       "400002     theresa_jx  @GiGisOssum :/ My entire friggin family?  Ther...   \n",
       "400003    mishellfish  I absolutely &lt;3 sleeping in on saturdays. I...   \n",
       "400004  meganleahhall                   is a little upset at the moment    \n",
       "\n",
       "                                               preprocess  \n",
       "400000                              so mutha effin bored   \n",
       "400001  AT_USER not at all...i've spent most of the af...  \n",
       "400002  AT_USER :/ my entire friggin family? there's n...  \n",
       "400003  i absolutely &lt;3 sleeping in on saturdays. i...  \n",
       "400004                   is a little upset at the moment   "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [target, id, date, Query, user, tweet, preprocess]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>Query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [target, id, date, Query, user, tweet, preprocess]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se remueven las url\n",
    "#df1.loc[:,'preprocess'] = df1.loc[:,'preprocess'].str.replace(\"https\\S+|http\\S+|www.\\S+\",\"\",case = False)\n",
    "#Se remueven las apostrofes\n",
    "#df1.loc[:,'preprocess'] = df1.loc[:,'preprocess'].str.replace(\"'\",\"\")\n",
    "#Se remueven los caracteres especiales\n",
    "#df1.loc[:,'preprocess'] = df1.loc[:,'preprocess'].str.replace(\"[^a-zA-Z#]\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se elminan todas las palabras de menos de 3 letras\n",
    "#df1.loc[:,'preprocess'] = df1.loc[:,'preprocess'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [AT_USER, URL, -, awww,, that's, a, bummer., y...\n",
       "1    [is, upset, that, he, can't, update, his, face...\n",
       "2    [AT_USER, i, dived, many, times, for, the, bal...\n",
       "3    [my, whole, body, feels, itchy, and, like, its...\n",
       "4    [AT_USER, no,, it's, not, behaving, at, all., ...\n",
       "Name: preprocess, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se convierten las palabras el tweet a elementos de un arreglo\n",
    "tokenized_tweet = df1.loc[:,'preprocess'].apply(lambda x: x.split())\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [at_us, url, -, awww,, that, a, bummer., you, ...\n",
       "1    [is, upset, that, he, can't, updat, his, faceb...\n",
       "2    [at_us, i, dive, mani, time, for, the, ball., ...\n",
       "3    [my, whole, bodi, feel, itchi, and, like, it, ...\n",
       "4    [at_us, no,, it, not, behav, at, all., i'm, ma...\n",
       "Name: preprocess, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "#stemmer = PorterStemmer()\n",
    "#Librer√¨a para manejar los verbos en sus distintas formas (falta validar si es buena opci√≥n)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Despues de transformar los verbos, los ponemos en el dataset\n",
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "\n",
    "df1.loc[:,'preprocess'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>at_us url - awww, that a bummer. you shoulda g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't updat his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>at_us i dive mani time for the ball. manag to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole bodi feel itchi and like it on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>at_us no, it not behav at all. i'm mad. whi am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>at_us not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Need a hug</td>\n",
       "      <td>need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "      <td>at_us hey long time no see! yes.. rain a bit ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "      <td>at_us nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "      <td>at_us que me muera ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spring break in plain city... it's snowing</td>\n",
       "      <td>spring break in plain city... it snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "      <td>i just re-pierc my ear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@caregiving I couldn't bear to watch it.  And ...</td>\n",
       "      <td>at_us i couldn't bear to watch it. and i thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@octolinz16 It it counts, idk why I did either...</td>\n",
       "      <td>at_us it it counts, idk whi i did either. you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@smarrison i would've been the first, but i di...</td>\n",
       "      <td>at_us i would'v been the first, but i didn't h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@iamjazzyfizzle I wish I got to watch it with ...</td>\n",
       "      <td>at_us i wish i got to watch it with you!! i mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "      <td>holli death scene will hurt me sever to watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>about to file tax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  lov...</td>\n",
       "      <td>at_us ahh ive alway want to see rent love the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@FakerPattyPattz Oh dear. Were you drinking ou...</td>\n",
       "      <td>at_us oh dear. were you drink out of the forgo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@alydesigns i was out most of the day so didn'...</td>\n",
       "      <td>at_us i was out most of the day so didn't get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>one of my friend called me, and asked to meet ...</td>\n",
       "      <td>one of my friend call me, and ask to meet with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@angry_barista I baked you a cake but I ated it</td>\n",
       "      <td>at_us i bake you a cake but i ate it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>this week is not going as i had hoped</td>\n",
       "      <td>this week is not go as i had hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "      <td>blagh class at 8 tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I hate when I have to call and wake people up</td>\n",
       "      <td>i hate when i have to call and wake peopl up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Just going to cry myself to sleep after watchi...</td>\n",
       "      <td>just go to cri myself to sleep after watch mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>im sad now  Miss.Lilly</td>\n",
       "      <td>im sad now miss.lilli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ooooh.... LOL  that leslie.... and ok I won't ...</td>\n",
       "      <td>ooooh.... lol that leslie.... and ok i won't d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Meh... Almost Lover is the exception... this t...</td>\n",
       "      <td>meh... almost lover is the exception... this t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199971</th>\n",
       "      <td>@heavensent2619 sorry for jettin out like that...</td>\n",
       "      <td>at_us sorri for jettin out like that but that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199972</th>\n",
       "      <td>OMG! I dnt wanna leave  I had soooooooo much f...</td>\n",
       "      <td>omg! i dnt wanna leav i had soooooooo much fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199973</th>\n",
       "      <td>Hungover, gonna stay in bed for a while. At le...</td>\n",
       "      <td>hungover, gonna stay in bed for a while. at le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199974</th>\n",
       "      <td>I woke up an hour after i was already supposed...</td>\n",
       "      <td>i woke up an hour after i was alreadi suppos t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199975</th>\n",
       "      <td>@mileycyrus please come to germany again</td>\n",
       "      <td>at_us pleas come to germani again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199976</th>\n",
       "      <td>helloo everyone! I'm at work until 5pm today a...</td>\n",
       "      <td>helloo everyone! i'm at work until 5pm today a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199977</th>\n",
       "      <td>There was a turtle in the pool this morning. M...</td>\n",
       "      <td>there was a turtl in the pool this morning. my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199978</th>\n",
       "      <td>Handed in dissertation yesterday morning...fol...</td>\n",
       "      <td>hand in dissert yesterday morning...follow by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199979</th>\n",
       "      <td>My polaroid failed.  - http://tweet.sg</td>\n",
       "      <td>my polaroid failed. - url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199980</th>\n",
       "      <td>has got so much stuff to do!!!...wanna be at t...</td>\n",
       "      <td>has got so much stuff to do!!!...wanna be at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199981</th>\n",
       "      <td>@marns_ ugh  mine's eased off nicely, hope you...</td>\n",
       "      <td>at_us ugh mine eas off nicely, hope your doe t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199982</th>\n",
       "      <td>I feel sad and lonely and I miss Kyle</td>\n",
       "      <td>i feel sad and lone and i miss kyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199983</th>\n",
       "      <td>@Sarah_1991  :@ It won't be for long. We'll be...</td>\n",
       "      <td>at_us :@ it won't be for long. we'll be back i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199984</th>\n",
       "      <td>@SLotH13 blame him for all bad jobs. He gave m...</td>\n",
       "      <td>at_us blame him for all bad jobs. he gave me a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199985</th>\n",
       "      <td>having trouble with my boyf</td>\n",
       "      <td>have troubl with my boyf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199986</th>\n",
       "      <td>@awaltzforanight aww *hugs* I know how that fe...</td>\n",
       "      <td>at_us aww *hugs* i know how that feel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199987</th>\n",
       "      <td>I think everyone but me is out in the sun.  No...</td>\n",
       "      <td>i think everyon but me is out in the sun. no t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199988</th>\n",
       "      <td>Is yard selling alone today  but someone gave ...</td>\n",
       "      <td>is yard sell alon today but someon gave us 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199989</th>\n",
       "      <td>@tinymicroserf I like the taste of prawns but ...</td>\n",
       "      <td>at_us i like the tast of prawn but can't eat t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199990</th>\n",
       "      <td>@uhohcaitie aw man I am jealous. can't believe...</td>\n",
       "      <td>at_us aw man i am jealous. can't believ i miss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199991</th>\n",
       "      <td>@warley I like it too, but i can't use the sea...</td>\n",
       "      <td>at_us i like it too, but i can't use the searc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199992</th>\n",
       "      <td>@loganculwell sorry I missed the movie last ni...</td>\n",
       "      <td>at_us sorri i miss the movi last night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199993</th>\n",
       "      <td>has had a very nasty migraine all day and has ...</td>\n",
       "      <td>has had a veri nasti migrain all day and has m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>with my main man. thankfully hes not crazy yet...</td>\n",
       "      <td>with my main man. thank hes not crazi yet. p.s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>Doesn't feel good.</td>\n",
       "      <td>doesn't feel good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>work... again</td>\n",
       "      <td>work... again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>@damienfranco Its so common for it to crash no...</td>\n",
       "      <td>at_us it so common for it to crash now i find ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>my baby boy is wearing big boy underwear</td>\n",
       "      <td>my babi boy is wear big boy underwear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>Fml! I forgot my phone charger @home!</td>\n",
       "      <td>fml! i forgot my phone charger at_us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200000</th>\n",
       "      <td>Can't believe I have to wait another 6 months ...</td>\n",
       "      <td>can't believ i have to wait anoth 6 month for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200001 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet  \\\n",
       "0       @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1       is upset that he can't update his Facebook by ...   \n",
       "2       @Kenichan I dived many times for the ball. Man...   \n",
       "3         my whole body feels itchy and like its on fire    \n",
       "4       @nationwideclass no, it's not behaving at all....   \n",
       "5                           @Kwesidei not the whole crew    \n",
       "6                                             Need a hug    \n",
       "7       @LOLTrish hey  long time no see! Yes.. Rains a...   \n",
       "8                    @Tatiana_K nope they didn't have it    \n",
       "9                               @twittera que me muera ?    \n",
       "10            spring break in plain city... it's snowing    \n",
       "11                             I just re-pierced my ears    \n",
       "12      @caregiving I couldn't bear to watch it.  And ...   \n",
       "13      @octolinz16 It it counts, idk why I did either...   \n",
       "14      @smarrison i would've been the first, but i di...   \n",
       "15      @iamjazzyfizzle I wish I got to watch it with ...   \n",
       "16      Hollis' death scene will hurt me severely to w...   \n",
       "17                                   about to file taxes    \n",
       "18      @LettyA ahh ive always wanted to see rent  lov...   \n",
       "19      @FakerPattyPattz Oh dear. Were you drinking ou...   \n",
       "20      @alydesigns i was out most of the day so didn'...   \n",
       "21      one of my friend called me, and asked to meet ...   \n",
       "22       @angry_barista I baked you a cake but I ated it    \n",
       "23                 this week is not going as i had hoped    \n",
       "24                             blagh class at 8 tomorrow    \n",
       "25         I hate when I have to call and wake people up    \n",
       "26      Just going to cry myself to sleep after watchi...   \n",
       "27                                 im sad now  Miss.Lilly   \n",
       "28      ooooh.... LOL  that leslie.... and ok I won't ...   \n",
       "29      Meh... Almost Lover is the exception... this t...   \n",
       "...                                                   ...   \n",
       "199971  @heavensent2619 sorry for jettin out like that...   \n",
       "199972  OMG! I dnt wanna leave  I had soooooooo much f...   \n",
       "199973  Hungover, gonna stay in bed for a while. At le...   \n",
       "199974  I woke up an hour after i was already supposed...   \n",
       "199975          @mileycyrus please come to germany again    \n",
       "199976  helloo everyone! I'm at work until 5pm today a...   \n",
       "199977  There was a turtle in the pool this morning. M...   \n",
       "199978  Handed in dissertation yesterday morning...fol...   \n",
       "199979             My polaroid failed.  - http://tweet.sg   \n",
       "199980  has got so much stuff to do!!!...wanna be at t...   \n",
       "199981  @marns_ ugh  mine's eased off nicely, hope you...   \n",
       "199982             I feel sad and lonely and I miss Kyle    \n",
       "199983  @Sarah_1991  :@ It won't be for long. We'll be...   \n",
       "199984  @SLotH13 blame him for all bad jobs. He gave m...   \n",
       "199985                       having trouble with my boyf    \n",
       "199986  @awaltzforanight aww *hugs* I know how that fe...   \n",
       "199987  I think everyone but me is out in the sun.  No...   \n",
       "199988  Is yard selling alone today  but someone gave ...   \n",
       "199989  @tinymicroserf I like the taste of prawns but ...   \n",
       "199990  @uhohcaitie aw man I am jealous. can't believe...   \n",
       "199991  @warley I like it too, but i can't use the sea...   \n",
       "199992  @loganculwell sorry I missed the movie last ni...   \n",
       "199993  has had a very nasty migraine all day and has ...   \n",
       "199994  with my main man. thankfully hes not crazy yet...   \n",
       "199995                                Doesn't feel good.    \n",
       "199996                                     work... again    \n",
       "199997  @damienfranco Its so common for it to crash no...   \n",
       "199998          my baby boy is wearing big boy underwear    \n",
       "199999             Fml! I forgot my phone charger @home!    \n",
       "200000  Can't believe I have to wait another 6 months ...   \n",
       "\n",
       "                                               preprocess  \n",
       "0       at_us url - awww, that a bummer. you shoulda g...  \n",
       "1       is upset that he can't updat his facebook by t...  \n",
       "2       at_us i dive mani time for the ball. manag to ...  \n",
       "3            my whole bodi feel itchi and like it on fire  \n",
       "4       at_us no, it not behav at all. i'm mad. whi am...  \n",
       "5                                at_us not the whole crew  \n",
       "6                                              need a hug  \n",
       "7       at_us hey long time no see! yes.. rain a bit ,...  \n",
       "8                          at_us nope they didn't have it  \n",
       "9                                    at_us que me muera ?  \n",
       "10                  spring break in plain city... it snow  \n",
       "11                                 i just re-pierc my ear  \n",
       "12      at_us i couldn't bear to watch it. and i thoug...  \n",
       "13      at_us it it counts, idk whi i did either. you ...  \n",
       "14      at_us i would'v been the first, but i didn't h...  \n",
       "15      at_us i wish i got to watch it with you!! i mi...  \n",
       "16      holli death scene will hurt me sever to watch ...  \n",
       "17                                      about to file tax  \n",
       "18      at_us ahh ive alway want to see rent love the ...  \n",
       "19      at_us oh dear. were you drink out of the forgo...  \n",
       "20      at_us i was out most of the day so didn't get ...  \n",
       "21      one of my friend call me, and ask to meet with...  \n",
       "22                   at_us i bake you a cake but i ate it  \n",
       "23                      this week is not go as i had hope  \n",
       "24                              blagh class at 8 tomorrow  \n",
       "25           i hate when i have to call and wake peopl up  \n",
       "26      just go to cri myself to sleep after watch mar...  \n",
       "27                                  im sad now miss.lilli  \n",
       "28      ooooh.... lol that leslie.... and ok i won't d...  \n",
       "29      meh... almost lover is the exception... this t...  \n",
       "...                                                   ...  \n",
       "199971  at_us sorri for jettin out like that but that ...  \n",
       "199972  omg! i dnt wanna leav i had soooooooo much fun...  \n",
       "199973  hungover, gonna stay in bed for a while. at le...  \n",
       "199974  i woke up an hour after i was alreadi suppos t...  \n",
       "199975                  at_us pleas come to germani again  \n",
       "199976  helloo everyone! i'm at work until 5pm today a...  \n",
       "199977  there was a turtl in the pool this morning. my...  \n",
       "199978  hand in dissert yesterday morning...follow by ...  \n",
       "199979                          my polaroid failed. - url  \n",
       "199980  has got so much stuff to do!!!...wanna be at t...  \n",
       "199981  at_us ugh mine eas off nicely, hope your doe t...  \n",
       "199982                i feel sad and lone and i miss kyle  \n",
       "199983  at_us :@ it won't be for long. we'll be back i...  \n",
       "199984  at_us blame him for all bad jobs. he gave me a...  \n",
       "199985                           have troubl with my boyf  \n",
       "199986              at_us aww *hugs* i know how that feel  \n",
       "199987  i think everyon but me is out in the sun. no t...  \n",
       "199988  is yard sell alon today but someon gave us 200...  \n",
       "199989  at_us i like the tast of prawn but can't eat t...  \n",
       "199990  at_us aw man i am jealous. can't believ i miss...  \n",
       "199991  at_us i like it too, but i can't use the searc...  \n",
       "199992            at_us sorri i miss the movi last night.  \n",
       "199993  has had a veri nasti migrain all day and has m...  \n",
       "199994  with my main man. thank hes not crazi yet. p.s...  \n",
       "199995                                 doesn't feel good.  \n",
       "199996                                      work... again  \n",
       "199997  at_us it so common for it to crash now i find ...  \n",
       "199998              my babi boy is wear big boy underwear  \n",
       "199999               fml! i forgot my phone charger at_us  \n",
       "200000  can't believ i have to wait anoth 6 month for ...  \n",
       "\n",
       "[200001 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.loc[:,\"tweet\":'preprocess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start replaceTwoOrMore\n",
    "def replace_two_or_more_letters(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start getStopWordList\n",
    "def get_dict_words_list(dictWordListFileName):\n",
    "    #read the stopwords file and build a list\n",
    "    stopWords = []\n",
    "    stopWords.append('AT_USER')\n",
    "    stopWords.append('URL')\n",
    "\n",
    "    fp = open(dictWordListFileName, 'r')\n",
    "    line = fp.readline()\n",
    "    while line:\n",
    "        word = line.strip()\n",
    "        stopWords.append(word)\n",
    "        line = fp.readline()\n",
    "    fp.close()\n",
    "    return stopWords\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start getfeatureVector\n",
    "def get_feature_vector(tweet):\n",
    "    featureVector = []\n",
    "    #split tweet into words\n",
    "    words = tweet.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences\n",
    "        w = replace_two_or_more_letters(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if the word stats with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stop word\n",
    "        if(w in stopWords or val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-af8bb52d74d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Graficar las palabras en una nube de palabras, actualmente no me deja instalar la librer√≠a por memoria\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'preprocess'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m110\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "#!pip install wordcloud\n",
    "#Graficar las palabras en una nube de palabras, actualmente no me deja instalar la librer√≠a por memoria\n",
    "all_words = ' '.join([text for text in df1.loc[:,'preprocess']])\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo swapon -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
