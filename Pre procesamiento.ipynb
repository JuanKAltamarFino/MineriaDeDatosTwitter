{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Laura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Laura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "#!pip install seaborn\n",
    "import seaborn as sns\n",
    "import string\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "import warnings \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1          is upset that he can't update his Facebook by ...\n",
      "2          @Kenichan I dived many times for the ball. Man...\n",
      "3            my whole body feels itchy and like its on fire \n",
      "4          @nationwideclass no, it's not behaving at all....\n",
      "5                              @Kwesidei not the whole crew \n",
      "6                                                Need a hug \n",
      "7          @LOLTrish hey  long time no see! Yes.. Rains a...\n",
      "8                       @Tatiana_K nope they didn't have it \n",
      "9                                  @twittera que me muera ? \n",
      "10               spring break in plain city... it's snowing \n",
      "11                                I just re-pierced my ears \n",
      "12         @caregiving I couldn't bear to watch it.  And ...\n",
      "13         @octolinz16 It it counts, idk why I did either...\n",
      "14         @smarrison i would've been the first, but i di...\n",
      "15         @iamjazzyfizzle I wish I got to watch it with ...\n",
      "16         Hollis' death scene will hurt me severely to w...\n",
      "17                                      about to file taxes \n",
      "18         @LettyA ahh ive always wanted to see rent  lov...\n",
      "19         @FakerPattyPattz Oh dear. Were you drinking ou...\n",
      "20         @alydesigns i was out most of the day so didn'...\n",
      "21         one of my friend called me, and asked to meet ...\n",
      "22          @angry_barista I baked you a cake but I ated it \n",
      "23                    this week is not going as i had hoped \n",
      "24                                blagh class at 8 tomorrow \n",
      "25            I hate when I have to call and wake people up \n",
      "26         Just going to cry myself to sleep after watchi...\n",
      "27                                    im sad now  Miss.Lilly\n",
      "28         ooooh.... LOL  that leslie.... and ok I won't ...\n",
      "29         Meh... Almost Lover is the exception... this t...\n",
      "                                 ...                        \n",
      "1599970    Thanks @eastwestchic &amp; @wangyip Thanks! Th...\n",
      "1599971    @marttn thanks Martin. not the most imaginativ...\n",
      "1599972            @MikeJonesPhoto Congrats Mike  Way to go!\n",
      "1599973    http://twitpic.com/7jp4n - OMG! Office Space.....\n",
      "1599974    @yrclndstnlvr ahaha nooo you were just away fr...\n",
      "1599975    @BizCoachDeb  Hey, I'm baack! And, thanks so m...\n",
      "1599976    @mattycus Yeah, my conscience would be clear i...\n",
      "1599977    @MayorDorisWolfe Thats my girl - dishing out t...\n",
      "1599978                            @shebbs123 i second that \n",
      "1599979                                       In the garden \n",
      "1599980    @myheartandmind jo jen by nemuselo zrovna tÃ© ...\n",
      "1599981    Another Commenting Contest! [;: Yay!!!  http:/...\n",
      "1599982    @thrillmesoon i figured out how to see my twee...\n",
      "1599983    @oxhot theri tomorrow, drinking coffee, talkin...\n",
      "1599984    You heard it here first -- We're having a girl...\n",
      "1599985    if ur the lead singer in a band, beware fallin...\n",
      "1599986                @tarayqueen too much ads on my blog. \n",
      "1599987    @La_r_a NEVEER  I think that you both will get...\n",
      "1599988    @Roy_Everitt ha- good job. that's right - we g...\n",
      "1599989                   @Ms_Hip_Hop im glad ur doing well \n",
      "1599990                                WOOOOO! Xbox is back \n",
      "1599991    @rmedina @LaTati Mmmm  That sounds absolutely ...\n",
      "1599992                    ReCoVeRiNg FrOm ThE lOnG wEeKeNd \n",
      "1599993                                    @SCOOBY_GRITBOYS \n",
      "1599994    @Cliff_Forster Yeah, that does work better tha...\n",
      "1599995    Just woke up. Having no school is the best fee...\n",
      "1599996    TheWDB.com - Very cool to hear old Walt interv...\n",
      "1599997    Are you ready for your MoJo Makeover? Ask me f...\n",
      "1599998    Happy 38th Birthday to my boo of alll time!!! ...\n",
      "1599999    happy #charitytuesday @theNSPCC @SparksCharity...\n",
      "Name: tweet, Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df=pd.read_csv(\"train_tweets.txt\",encoding='latin' , names=[\"target\",\"id\",\"date\",\"Query\",\"user\",\"tweet\"])\n",
    "#print(df['tweet'])\n",
    "low_memory=False\n",
    "#df.head(9)\n",
    "df.shape\n",
    "print(df['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metodo que remueve un patrón usado para quitar las menciones @user\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)    \n",
    "    return input_txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se elminan las menciones del dataset\n",
    "df.loc[:,'preprocess'] = np.vectorize(remove_pattern)(df['tweet'], \"@[\\w]*\")\n",
    "#Se remueven las url\n",
    "df.loc[:,'preprocess'] = df.loc[:,'preprocess'].str.replace(\"https\\S+|http\\S+|www.\\S+\",\"\",case = False)\n",
    "#Se remueven las apostrofes\n",
    "df.loc[:,'preprocess'] = df.loc[:,'preprocess'].str.replace(\"'\",\"\")\n",
    "#Se remueven los caracteres especiales\n",
    "df.loc[:,'preprocess'] = df.loc[:,'preprocess'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
    "#Convert to lower case\n",
    "df.loc[:,'preprocess'] = df.loc[:,'preprocess'].str.lower()\n",
    "#Convert @username to AT_USER\n",
    "df.loc[:,'preprocess'] = df.loc[:,'preprocess'].str.replace('@[^\\s]+', 'AT_USER', regex=True)\n",
    "#Se elminan todas las palabras de menos de 3 letras\n",
    "#df.loc[:,'preprocess'] = df.loc[:,'preprocess'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>awww  thats a bummer   you shoulda got dav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball  managed to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no  its not behaving at all  im mad  why am i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                          preprocess  \n",
       "0      awww  thats a bummer   you shoulda got dav...  \n",
       "1  is upset that he cant update his facebook by t...  \n",
       "2   i dived many times for the ball  managed to s...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4   no  its not behaving at all  im mad  why am i...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Se usa para visualizar el preprocess del dataset\n",
    "df.head().loc[:,\"tweet\":\"preprocess\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wasn', \"she's\", 'won', 'haven', 'from', 'should', 'been', 'the', 'our', 'after', 'under', \"shan't\", 'she', 'are', 'further', 'isn', \"don't\", 'weren', 've', 'and', 'needn', 'yourselves', 'himself', 'all', 'hers', 'be', 'where', 'll', 'aren', 'very', \"shouldn't\", 'against', \"couldn't\", 'up', 'through', 'than', 'its', 'has', 'didn', \"won't\", 'theirs', 'this', 'other', 'until', 'about', 'of', 'whom', \"hasn't\", 'him', 'both', 'shan', 'as', 'now', 'hadn', 'not', 'their', 'only', 't', \"doesn't\", 'm', 'by', \"should've\", 'these', 'ain', 'what', 'because', 'your', \"didn't\", 'who', 'why', \"hadn't\", 'during', \"isn't\", 'mightn', 'on', 'in', 'wouldn', 'nor', 'but', 'i', 'her', 'then', 'most', 'ma', \"you're\", 'or', 'into', 're', 'between', 'each', 'own', 'if', 'few', 'to', 'hasn', 'at', 'it', 'having', 'a', 'will', 'any', 'which', 'itself', 'above', 'his', \"it's\", 'that', 'yourself', 'me', 's', 'again', 'they', 'ourselves', 'am', 'those', \"that'll\", 'once', 'being', 'before', \"weren't\", \"mightn't\", \"you've\", 'some', \"haven't\", \"you'll\", 'have', 'same', 'mustn', 'down', 'with', 'he', 'you', 'was', 'does', 'out', 'more', 'don', 'when', \"wouldn't\", 'doing', 'herself', 'here', 'there', 'd', 'an', 'over', \"wasn't\", 'below', 'so', 'my', 'o', 'doesn', \"mustn't\", \"you'd\", 'themselves', 'no', 'yours', 'is', 'couldn', 'such', \"aren't\", 'ours', 'did', 'for', 'we', 'were', 'them', 'can', 'y', 'just', 'do', 'how', 'had', 'while', \"needn't\", 'shouldn', 'myself', 'off', 'too'}\n"
     ]
    }
   ],
   "source": [
    "#Listado de stop_words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          [awww, thats, a, bummer, you, shoulda, got, da...\n",
      "1          [is, upset, that, he, cant, update, his, faceb...\n",
      "2          [i, dived, many, times, for, the, ball, manage...\n",
      "3          [my, whole, body, feels, itchy, and, like, its...\n",
      "4          [no, its, not, behaving, at, all, im, mad, why...\n",
      "5                                    [not, the, whole, crew]\n",
      "6                                             [need, a, hug]\n",
      "7          [hey, long, time, no, see, yes, rains, a, bit,...\n",
      "8                              [nope, they, didnt, have, it]\n",
      "9                                           [que, me, muera]\n",
      "10            [spring, break, in, plain, city, its, snowing]\n",
      "11                          [i, just, re, pierced, my, ears]\n",
      "12         [i, couldnt, bear, to, watch, it, and, i, thou...\n",
      "13         [it, it, counts, idk, why, i, did, either, you...\n",
      "14         [i, wouldve, been, the, first, but, i, didnt, ...\n",
      "15         [i, wish, i, got, to, watch, it, with, you, i,...\n",
      "16         [hollis, death, scene, will, hurt, me, severel...\n",
      "17                                  [about, to, file, taxes]\n",
      "18         [ahh, ive, always, wanted, to, see, rent, love...\n",
      "19         [oh, dear, were, you, drinking, out, of, the, ...\n",
      "20         [i, was, out, most, of, the, day, so, didnt, g...\n",
      "21         [one, of, my, friend, called, me, and, asked, ...\n",
      "22                [i, baked, you, a, cake, but, i, ated, it]\n",
      "23           [this, week, is, not, going, as, i, had, hoped]\n",
      "24                              [blagh, class, at, tomorrow]\n",
      "25         [i, hate, when, i, have, to, call, and, wake, ...\n",
      "26         [just, going, to, cry, myself, to, sleep, afte...\n",
      "27                               [im, sad, now, miss, lilly]\n",
      "28         [ooooh, lol, that, leslie, and, ok, i, wont, d...\n",
      "29         [meh, almost, lover, is, the, exception, this,...\n",
      "                                 ...                        \n",
      "1599970    [thanks, amp, thanks, that, was, just, what, i...\n",
      "1599971    [thanks, martin, not, the, most, imaginative, ...\n",
      "1599972                        [congrats, mike, way, to, go]\n",
      "1599973            [omg, office, space, i, wanna, steal, it]\n",
      "1599974    [ahaha, nooo, you, were, just, away, from, eve...\n",
      "1599975    [hey, im, baack, and, thanks, so, much, for, a...\n",
      "1599976    [yeah, my, conscience, would, be, clear, in, t...\n",
      "1599977    [thats, my, girl, dishing, out, the, quot, adv...\n",
      "1599978                                    [i, second, that]\n",
      "1599979                                    [in, the, garden]\n",
      "1599980    [jo, jen, by, nemuselo, zrovna, t, holce, ael,...\n",
      "1599981                  [another, commenting, contest, yay]\n",
      "1599982    [i, figured, out, how, to, see, my, tweets, an...\n",
      "1599983    [theri, tomorrow, drinking, coffee, talking, a...\n",
      "1599984    [you, heard, it, here, first, were, having, a,...\n",
      "1599985    [if, ur, the, lead, singer, in, a, band, bewar...\n",
      "1599986                       [too, much, ads, on, my, blog]\n",
      "1599987    [neveer, i, think, that, you, both, will, get,...\n",
      "1599988    [ha, good, job, thats, right, we, gotta, throw...\n",
      "1599989                          [im, glad, ur, doing, well]\n",
      "1599990                             [wooooo, xbox, is, back]\n",
      "1599991    [mmmm, that, sounds, absolutely, perfect, but,...\n",
      "1599992               [recovering, from, the, long, weekend]\n",
      "1599993                                                   []\n",
      "1599994    [yeah, that, does, work, better, than, just, w...\n",
      "1599995    [just, woke, up, having, no, school, is, the, ...\n",
      "1599996    [thewdb, com, very, cool, to, hear, old, walt,...\n",
      "1599997    [are, you, ready, for, your, mojo, makeover, a...\n",
      "1599998    [happy, th, birthday, to, my, boo, of, alll, t...\n",
      "1599999                             [happy, #charitytuesday]\n",
      "Name: word_tokens, Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Se convierten las palabras el tweet a elementos de un arreglo\n",
    "df['word_tokens']=df.loc[:,'preprocess'].apply(TweetTokenizer().tokenize)\n",
    "print(df['word_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          [awww, thats, bummer, shoulda, got, david, car...\n",
      "1          [upset, cant, update, facebook, texting, might...\n",
      "2          [dived, many, times, ball, managed, save, rest...\n",
      "3                    [whole, body, feels, itchy, like, fire]\n",
      "4                             [behaving, im, mad, cant, see]\n",
      "5                                              [whole, crew]\n",
      "6                                                [need, hug]\n",
      "7          [hey, long, time, see, yes, rains, bit, bit, l...\n",
      "8                                              [nope, didnt]\n",
      "9                                               [que, muera]\n",
      "10                     [spring, break, plain, city, snowing]\n",
      "11                                           [pierced, ears]\n",
      "12         [couldnt, bear, watch, thought, ua, loss, emba...\n",
      "13               [counts, idk, either, never, talk, anymore]\n",
      "14         [wouldve, first, didnt, gun, really, though, z...\n",
      "15                        [wish, got, watch, miss, premiere]\n",
      "16         [hollis, death, scene, hurt, severely, watch, ...\n",
      "17                                             [file, taxes]\n",
      "18         [ahh, ive, always, wanted, see, rent, love, so...\n",
      "19            [oh, dear, drinking, forgotten, table, drinks]\n",
      "20                             [day, didnt, get, much, done]\n",
      "21         [one, friend, called, asked, meet, mid, valley...\n",
      "22                                       [baked, cake, ated]\n",
      "23                                      [week, going, hoped]\n",
      "24                                  [blagh, class, tomorrow]\n",
      "25                                [hate, call, wake, people]\n",
      "26                     [going, cry, sleep, watching, marley]\n",
      "27                                    [im, sad, miss, lilly]\n",
      "28         [ooooh, lol, leslie, ok, wont, leslie, wont, g...\n",
      "29         [meh, almost, lover, exception, track, gets, d...\n",
      "                                 ...                        \n",
      "1599970                       [thanks, amp, thanks, looking]\n",
      "1599971       [thanks, martin, imaginative, interface, itll]\n",
      "1599972                            [congrats, mike, way, go]\n",
      "1599973                   [omg, office, space, wanna, steal]\n",
      "1599974    [ahaha, nooo, away, everyone, else, see, kara,...\n",
      "1599975    [hey, im, baack, thanks, much, kind, notes, go...\n",
      "1599976               [yeah, conscience, would, clear, case]\n",
      "1599977           [thats, girl, dishing, quot, advice, quot]\n",
      "1599978                                             [second]\n",
      "1599979                                             [garden]\n",
      "1599980     [jo, jen, nemuselo, zrovna, holce, ael, co, nic]\n",
      "1599981                  [another, commenting, contest, yay]\n",
      "1599982    [figured, see, tweets, facebook, status, updat...\n",
      "1599983    [theri, tomorrow, drinking, coffee, talking, i...\n",
      "1599984    [heard, first, girl, hope, looks, wendys, brai...\n",
      "1599985    [ur, lead, singer, band, beware, falling, prey...\n",
      "1599986                                    [much, ads, blog]\n",
      "1599987                           [neveer, think, get, well]\n",
      "1599988    [ha, good, job, thats, right, gotta, throw, #b...\n",
      "1599989                                 [im, glad, ur, well]\n",
      "1599990                                 [wooooo, xbox, back]\n",
      "1599991    [mmmm, sounds, absolutely, perfect, schedule, ...\n",
      "1599992                          [recovering, long, weekend]\n",
      "1599993                                                   []\n",
      "1599994    [yeah, work, better, waiting, end, wonder, tim...\n",
      "1599995                  [woke, school, best, feeling, ever]\n",
      "1599996     [thewdb, com, cool, hear, old, walt, interviews]\n",
      "1599997                [ready, mojo, makeover, ask, details]\n",
      "1599998    [happy, th, birthday, boo, alll, time, tupac, ...\n",
      "1599999                             [happy, #charitytuesday]\n",
      "Name: without_stop_words, Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['without_stop_words']=df['word_tokens'].apply(lambda x: [i for i in x if i.lower() not in stop_words])\n",
    "print(df['without_stop_words'])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>preprocess</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>without_stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>awww  thats a bummer   you shoulda got dav...</td>\n",
       "      <td>[awww, thats, a, bummer, you, shoulda, got, da...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball  managed to s...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "      <td>[dived, many, times, ball, managed, save, rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no  its not behaving at all  im mad  why am i...</td>\n",
       "      <td>[no, its, not, behaving, at, all, im, mad, why...</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1  is upset that he can't update his Facebook by ...   \n",
       "2  @Kenichan I dived many times for the ball. Man...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                          preprocess  \\\n",
       "0      awww  thats a bummer   you shoulda got dav...   \n",
       "1  is upset that he cant update his facebook by t...   \n",
       "2   i dived many times for the ball  managed to s...   \n",
       "3    my whole body feels itchy and like its on fire    \n",
       "4   no  its not behaving at all  im mad  why am i...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [awww, thats, a, bummer, you, shoulda, got, da...   \n",
       "1  [is, upset, that, he, cant, update, his, faceb...   \n",
       "2  [i, dived, many, times, for, the, ball, manage...   \n",
       "3  [my, whole, body, feels, itchy, and, like, its...   \n",
       "4  [no, its, not, behaving, at, all, im, mad, why...   \n",
       "\n",
       "                                  without_stop_words  \n",
       "0  [awww, thats, bummer, shoulda, got, david, car...  \n",
       "1  [upset, cant, update, facebook, texting, might...  \n",
       "2  [dived, many, times, ball, managed, save, rest...  \n",
       "3            [whole, body, feels, itchy, like, fire]  \n",
       "4                     [behaving, im, mad, cant, see]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().loc[:,\"tweet\":\"without_stop_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
